<!DOCTYPE html>
<html lang="en">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-97010688-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-97010688-1');
</script>


  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Un Supervised NMT</title>
  <meta name="description" content="Research Summaries">

  <link rel="canonical" href="https://thammegowda.github.io/summary/nmt/content/03-unsup/01-unsupervised-nmt.html">
  <link rel="alternate" type="application/rss+xml" title="Research Summaries" href="https://thammegowda.github.io/summary/nmt/feed.xml">

  <meta property="og:url"         content="https://thammegowda.github.io/summary/nmt/content/03-unsup/01-unsupervised-nmt.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Un Supervised NMT" />
<meta property="og:description" content="Research Summaries" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "https://thammegowda.github.io/summary/nmt/content/03-unsup/01-unsupervised-nmt.html",
  "headline":
    "Un Supervised NMT",
  "datePublished":
    "2019-06-13T14:48:52-07:00",
  "dateModified":
    "2019-06-13T14:48:52-07:00",
  "description":
    "Research Summaries",
  "author": {
    "@type": "Person",
    "name": "Thamme Gowda"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://thammegowda.github.io/summary/nmt",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://thammegowda.github.io/summary/nmt",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/summary/nmt/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/summary/nmt/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/summary/nmt';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/summary/nmt/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/summary/nmt/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
/**
 * Set up thebelab button for code blocks
 */

const thebelabCellButton = id =>
  `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="/summary/nmt/assets/images/edit-button.svg" alt="Start interactive mode">
  </a>`


const addThebelabButtonToCodeCells =  () => {

  const codeCells = document.querySelectorAll('div.input_area > div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("thebelab-cell-button-" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
    }
  })
}

initFunction(addThebelabButtonToCodeCells);
</script>



<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'jupyter/jupyter-book',
        ref: 'gh-pages',
      },
      codeMirrorConfig: {
        theme: "abcdef"
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.4.0/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>



  <!-- Load the auto-generating TOC -->
  <script src="/summary/nmt/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/summary/nmt/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/summary/nmt/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/summary/nmt/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/summary/nmt/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/summary/nmt/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("https://thammegowda.github.io") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/summary/nmt/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/intro.html"><img src="/summary/nmt/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">Research Summaries</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/intro.html"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/summary/nmt/search.html">Search</a></li>
        
      
      
        <li class="c-sidebar__divider"></li>
        
      
      
        <li><h2 class="c-sidebar__title">Neural Machine Translation</li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/00-background/01-background.html"
        >
          
            1.
          
          Background
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/summary/nmt/00-background/02-variational-auto-encoder.html"
                >
                  
                    1.1
                  
                  Variational Auto Encoder
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/summary/nmt/00-background/03-gans.html"
                >
                  
                    1.2
                  
                  GANs
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/01-sup/01-beginning.html"
        >
          
            2.
          
          Supervised MT
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/02-semisup/01-semisup.html"
        >
          
            3.
          
          Semi Supervised MT
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry c-sidebar__entry--active"
          href="/summary/nmt/03-unsup/01-unsupervised-nmt.html"
        >
          
            4.
          
          Un-Supervised MT
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
      <aside class="sidebar__right">
          <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
          <nav class="onthispage">
          </nav>
      </aside>
      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            

            <div class="c-textbook__content">
              <h1 id="un-supervised-nmt">Un Supervised NMT</h1>

<p>Training NMT models without using a single parallel data sounds like a daunting challenge at first - how is it even possible?
There were efforts to train Statistical MT systems without using bitext, notably the paper titled <a href="https://www.aclweb.org/anthology/P11-1002">Deciphering foreign language</a> <a href="#ravi2011deciphering">(Ravi &amp; Knight, 2011)</a>.
In NMT, we see a natural progression of the task in step-by-step manner: first, learn word translations without any data, next, use those aligned embeddings to for sentence translations.</p>

<p>Unsupervised Word alignments, done in embedding space, is generally a two step process:</p>
<ol>
  <li>Learn monolingual embeddings seperately</li>
  <li>Learn the transformation function mapping one space to another.</li>
</ol>

<p>There are plenty of ways to learn word embeddings: Word2vec, Fasttext, Glove, etc Each have their own advantages (and some drawbacks). The task is straight forward application of distributional hypothesis on monolingual data.
The second step, learning transformation function for mapping one to another is some what interesting for translation community.</p>

<p>Progression of learning word embedding alignments:</p>
<ol>
  <li>Using a dictionary of word translations to learn the transformation matrix. <a href="https://arxiv.org/pdf/1309.4168.pdf">Exploiting Similarities among Languages for Machine Translation</a> <a href="#mikolov2013exploiting">(Mikolov, Le, &amp; Sutskever, 2013)</a></li>
  <li>Much smaller dictionaries: as small as 25 pairs. In many cases those pairs can be automatically obtained, eg: numbers, names etc. See <a href="https://www.aclweb.org/anthology/P17-1042">Learning bilingual word embeddings with (almost) no bilingual data</a> <a href="#artetxe2017binlingemb">(Artetxe, Labaka, &amp; Agirre, 2017)</a></li>
  <li>Fully unsupervised using more advanced techniques for word alignments: Using adversarial training. See <a href="https://arxiv.org/pdf/1710.04087.pdf">Word translation without parallel data</a> <a href="#conneau2017word">(Conneau, Lample, Ranzato, Denoyer, &amp; Jégou, 2017)</a> .</li>
</ol>

<p>The implementation of <a href="#conneau2017word">(Conneau, Lample, Ranzato, Denoyer, &amp; Jégou, 2017)</a> ‘s approch is made available on github as <a href="https://github.com/facebookresearch/MUSE">facebookresearch/MUSE</a> and it is more popular. These unsupervised approaches can sometimes perform better than supervised approach. Here is a visual interepretation of embedding alignment (taken from their github repo): <img src="https://github.com/facebookresearch/MUSE/raw/master/outline_all.png" alt="" /></p>

<p>In summary, unsupervised word alignments exploited these two phenomenons:</p>
<ol>
  <li>Words having similar meaning appear in similar context across languages</li>
  <li>There is a linear mapping from one embedding vector space to another which can be easily learned</li>
</ol>

<p>After having the automatically aligned word embeddings of good enough quality, the next problem to tackle was: “how to do sentence translation without parallel data?”</p>

<h3 id="1-unsupervised-neural-machine-translation-artetxe-labaka-agirre--cho-2017">1. <a href="https://arxiv.org/pdf/1710.11041.pdf">Unsupervised neural machine translation</a> <a href="#artetxe2017unsupervised">(Artetxe, Labaka, Agirre, &amp; Cho, 2017)</a></h3>

<ul>
  <li>Crosslingual embeddings: Skipgram word2vec, 10 neg sample, 10 ctx window, 300 dims. Then aligned using <a href="#artetxe2017binlingemb">(Artetxe, Labaka, &amp; Agirre, 2017)</a></li>
  <li>NMT Architecture: 2 layer Bi-GRU encoder, 2 layers GRU dec, 600 hid dim, 300 dim embs, general attention
    <ul>
      <li>Both directions: Source -&gt; Target and Target -&gt; Source</li>
      <li>Shared encoder for both source and target, decoders are seperate. Encoder embeddings are fixed (pretrained, aligned). Decoders are let to evolve.</li>
      <li>Vocabularies are separate for both languages. So embedding matrices are seperate (but aligned)</li>
    </ul>
  </li>
  <li>Denoising: for a seq of $N$ toks, $N/2$ swaps are performed as nimitation of oise. Without noise, copy task is too trivial, AEs doesnt learn any useful representation.</li>
  <li>On-the-fly back translation: they use the model to generate translation (in the inference mode with greedy dec) then reconstruct from the translation. Backtranslation is much noisier form than random word swaps.</li>
  <li>Switch the training steps: between Denoise L1 to L1; Denoise L2 to L2; Cycle via BackTranslation: L1 -&gt; L2 -&gt; L1; Cycle via BackTranslation: L2 -&gt; L1 -&gt; L2.</li>
  <li>Trained on short seqs: 50 or fewer toks (after BPE)</li>
  <li>Crosslingual embeddings:</li>
  <li>Took 4-5 days to train. Results on WMT: Fr-en 15.6; De-en 10.2</li>
</ul>

<h3 id="2-unsupervised-machine-translation-using-monolingual-corpora-only-lample-conneau-denoyer--ranzato-2017">2. <a href="https://arxiv.org/pdf/1711.00043.pdf">Unsupervised machine translation using monolingual corpora only</a> <a href="#lample2017unsupervised">(Lample, Conneau, Denoyer, &amp; Ranzato, 2017)</a></h3>

<ul>
  <li>Cross lingual embeddings from <a href="#conneau2017word">(Conneau, Lample, Ranzato, Denoyer, &amp; Jégou, 2017)</a> i.e. MUSE.</li>
  <li>Learn to reconstruct in both langs from the shared latent space</li>
  <li>Denoising auto encoder, reconstruct from a noisy input</li>
  <li>
    <p>(This paper has good citations to relevant work; semi supervised, autoencoder etc)</p>
  </li>
  <li>LSTM with 300 dims, 3 layers. Two models: Src-to-tgt and tgt-to-src models: all encoder layers are shared, all decoder layers are shared. I assume embeddings are frozen (pretrained, algined). So only the generator matrix is different for both the languages (they are not tied to input embeddings unlike the recent trend). They actually have two models, but LSTM layers are shared.</li>
  <li>Denoising auto encoders: drop words, shuffle the order of tokens (upper bound k on how many timesteps max a token can move). They found 10% word drop and k=3 be good parameters</li>
  <li>Cross-domain loss (aka cycle loss) x -&gt; C(x) -&gt; y -&gt; C(y) -&gt; x’.     C is corruption operator. Similarly, y -&gt; C(y) -&gt; x -&gt; C(x) -&gt; y’.</li>
  <li>Adversarial training: Encoding of x and y sentences should be indistinguishable</li>
  <li>Final objective is: is linear combination of following
    <ul>
      <li>AE src-&gt;src</li>
      <li>AE tgt-&gt;tgt</li>
      <li>Cross Domain aka CycleLoss src-&gt;tgt-&gt;src</li>
      <li>Cross Domain aka CycleLoss tgt-&gt;src-&gt;src</li>
      <li>Adversarial Loss</li>
    </ul>
  </li>
  <li>Note: for the first epoch, the Cross Domain uses word-by-word translation using MUSE; then later they switch to the model itself as backtranslation</li>
  <li>Results On WMT fr-en 14.3 while supervised is 26.1; de-en is 13.3 while supervvised is 25.6</li>
</ul>

<h3 id="3-unsupervised-neural-machine-translation-with-weight-sharing-yang-chen-wang--xu-2018">3. <a href="https://arxiv.org/pdf/1804.09057.pdf">Unsupervised neural machine translation with weight sharing</a> <a href="#yang2018unsupervised">(Yang, Chen, Wang, &amp; Xu, 2018)</a></h3>

<ul>
  <li><a href="#artetxe2017unsupervised">(Artetxe, Labaka, Agirre, &amp; Cho, 2017)</a> use a shared encoder; but seperate decoders. <a href="#lample2017unsupervised">(Lample, Conneau, Denoyer, &amp; Ranzato, 2017)</a> share a single encoder and single decoder (fully shared).</li>
  <li>Conjecture: Fully shared is not  good, since languages have different syntax; completely different is also not good, since forcing common latent space is hard. We want to something in the middle: partial sharing.</li>
  <li>Encoder: share the higher layers; allow the lower layers (close to embeddings) to be different</li>
  <li>Decoder share the lower layers; allow the higher layers (close to generator module) to be different
    <ul>
      <li>Question: Decoder too has input embedding too just like encoder, but they are okay shared? Maybe: since word embeddings are in common latent space</li>
    </ul>
  </li>
  <li>Two kinds of adversarial discriminators were built in:
    <ul>
      <li>Prior work (Yang et all 2017, same first author): “Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets”</li>
      <li>Loss is modified to include regular loss + GAN reward</li>
      <li>Two Discriminators: Local (encoded repr of source &amp; target are indistinguishable), Global (model generated sentences are indistinguishable from human generated sentences)</li>
    </ul>
  </li>
  <li>Slight improvement in BLEU (+1.0) compared to Lample et al 2017</li>
</ul>

<h3 id="4-phrase-based--neural-unsupervised-machine-translationlample-ott-conneau-denoyer--ranzato-2018">4. <a href="https://www.aclweb.org/anthology/D18-1549">Phrase-Based &amp; Neural Unsupervised Machine Translation</a><a href="#lample2018unsup-mt">(Lample, Ott, Conneau, Denoyer, &amp; Ranzato, 2018)</a></h3>

<ul>
  <li>Simplification from lample et al 2017 and Artetxe et al 2017. In essence these three work for unsupervised MT (ablation study proved it):
    <ul>
      <li>Initialization of embeddings via crosslingual embeddings</li>
      <li>strong language models via denoising autoencoder</li>
      <li>automatic backtranslation</li>
    </ul>
  </li>
  <li>They train NMT and PBSMT (moses). Source code at <a href="https://github.com/facebookresearch/UnsupervisedMT">facebookresearch/UnsupervisedMT</a></li>
  <li>NMT training
    <ul>
      <li>two varieties: LSTM (told that the setup is same as Artetxe et al 2017 but they had used GRUs!), Transformer (4 layers); 512 dim.</li>
      <li>Shared Encoder, forced interlingua using adversarial loss. Shared Decoder too for regularization</li>
      <li>BOS (first token) of the shared decoder is the language id token. Encoder’s first token is not modified unlike Johnson et al 2016.</li>
      <li>Loss is Denoising reconstruction + backtranslation loss ; Note: No adversarial loss ? Maybe bcoz the encoder is shared, no need to explicitly force it to learn interlingua</li>
    </ul>
  </li>
  <li>NOTE:  the weights of 3 out of 4 layers were shared as suggested by Yang et al 2018 (see above) (as seen in code, not clear from the paper).</li>
  <li>Validation and model selection via roundtrip BLEU ( x -&gt; y -&gt; x;  y-&gt; x-&gt;y).</li>
  <li>Note: Roundtrip BLEU correlates well for transformer but not for LSTMs, nobody knows why! They used a small validation set of 100 parallel sentences for LSTMs validation</li>
</ul>

<h3 id="cross-lingual-language-model-pretraining-lample--conneau-2019"><a href="https://arxiv.org/pdf/1901.07291.pdf">Cross-lingual Language Model Pretraining</a> <a href="#DBLP:journals/corr/abs-1901-07291">(Lample &amp; Conneau, 2019)</a></h3>
<ul>
  <li>Code is made available on gitub at <a href="https://github.com/facebookresearch/XLM">facebookresearch/XLM</a> . (Very well written!)</li>
  <li>Improvements over lample et al 2018; better initialization</li>
  <li>Instead of  initializing (just) embeddings they train language models encoders with different objectives: causal LM (predict next token given left context); masked LM predict some masked token in sequence given both left and right context</li>
  <li>If parallel data is available, it can be used to do Translation LM; concat src + tgt sequence, mask out some sequences on both source and target. Nice!!</li>
  <li>When training Language Models: Mix all the languages; adjust sampling to balance low and high resources</li>
  <li>Unlike earlier works which only used embeddings for auto encoders; this model initialized encoder and decoder layers from Cross Lingual Language model(XLM).</li>
  <li>XLM is a BERT like transformer model trained using Masked LM. Embeddings are sum of word emb, postional emb, language embeddings</li>
  <li>Embeddings, Encoder layers, Generation Matrix weghts are all compatible from XML  to NMT’s encoder</li>
  <li>Decoder gets most of the weights; misses source-attention weights from XLM,  but thats okay!</li>
  <li>Denoising AE and online backtranslation are still the key components</li>
  <li>High BLEU scores comparable with supervised MT (going to try this)</li>
  <li>Computationally expensive: Language models are way expensive (they needed 64 top of the class GPUs), MT finetuning is relatively inexpensive however still expensive (they needed 8 GPUs)</li>
</ul>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="ravi2011deciphering">Ravi, S., &amp; Knight, K. (2011). Deciphering foreign language. In <i>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</i> (pp. 12–21).</span></li>
<li><span id="mikolov2013exploiting">Mikolov, T., Le, Q. V., &amp; Sutskever, I. (2013). Exploiting similarities among languages for machine translation. <i>ArXiv Preprint ArXiv:1309.4168</i>.</span></li>
<li><span id="artetxe2017binlingemb">Artetxe, M., Labaka, G., &amp; Agirre, E. (2017). Learning bilingual word embeddings with (almost) no bilingual data. In <i>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i> (pp. 451–462).</span></li>
<li><span id="conneau2017word">Conneau, A., Lample, G., Ranzato, M. A., Denoyer, L., &amp; Jégou, H. (2017). Word translation without parallel data. <i>ArXiv Preprint ArXiv:1710.04087</i>.</span></li>
<li><span id="artetxe2017unsupervised">Artetxe, M., Labaka, G., Agirre, E., &amp; Cho, K. (2017). Unsupervised neural machine translation. <i>ArXiv Preprint ArXiv:1710.11041</i>.</span></li>
<li><span id="lample2017unsupervised">Lample, G., Conneau, A., Denoyer, L., &amp; Ranzato, M. A. (2017). Unsupervised machine translation using monolingual corpora only. <i>ArXiv Preprint ArXiv:1711.00043</i>.</span></li>
<li><span id="yang2018unsupervised">Yang, Z., Chen, W., Wang, F., &amp; Xu, B. (2018). Unsupervised neural machine translation with weight sharing. <i>ArXiv Preprint ArXiv:1804.09057</i>.</span></li>
<li><span id="lample2018unsup-mt">Lample, G., Ott, M., Conneau, A., Denoyer, L., &amp; Ranzato, M. A. (2018). Phrase-based &amp; neural unsupervised machine translation. <i>ArXiv Preprint ArXiv:1804.07755</i>.</span></li>
<li><span id="DBLP:journals/corr/abs-1901-07291">Lample, G., &amp; Conneau, A. (2019). Cross-lingual Language Model Pretraining. <i>CoRR</i>, <i>abs/1901.07291</i>. Retrieved from http://arxiv.org/abs/1901.07291</span></li></ol>

              <nav class="c-page__nav">
  

  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
