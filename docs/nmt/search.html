<!DOCTYPE html>
<html lang="en">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-97010688-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-97010688-1');
</script>


  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Search the site</title>
  <meta name="description" content="Research Summaries">

  <link rel="canonical" href="https://thammegowda.github.io/summary/nmt/search">
  <link rel="alternate" type="application/rss+xml" title="Research Summaries" href="https://thammegowda.github.io/summary/nmt/feed.xml">

  <meta property="og:url"         content="https://thammegowda.github.io/summary/nmt/search" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Search the site" />
<meta property="og:description" content="Research Summaries" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "https://thammegowda.github.io/summary/nmt/search",
  "headline":
    "Search the site",
  "datePublished":
    "2019-06-11T18:42:50-07:00",
  "dateModified":
    "2019-06-11T18:42:50-07:00",
  "description":
    "Research Summaries",
  "author": {
    "@type": "Person",
    "name": "Thamme Gowda"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://thammegowda.github.io/summary/nmt",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://thammegowda.github.io/summary/nmt",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/summary/nmt/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/summary/nmt/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/summary/nmt';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/summary/nmt/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/summary/nmt/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
/**
 * Set up thebelab button for code blocks
 */

const thebelabCellButton = id =>
  `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="/summary/nmt/assets/images/edit-button.svg" alt="Start interactive mode">
  </a>`


const addThebelabButtonToCodeCells =  () => {

  const codeCells = document.querySelectorAll('div.input_area > div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("thebelab-cell-button-" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
    }
  })
}

initFunction(addThebelabButtonToCodeCells);
</script>



<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'jupyter/jupyter-book',
        ref: 'gh-pages',
      },
      codeMirrorConfig: {
        theme: "abcdef"
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.4.0/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>



  <!-- Load the auto-generating TOC -->
  <script src="/summary/nmt/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/summary/nmt/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/summary/nmt/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/summary/nmt/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/summary/nmt/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/summary/nmt/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("https://thammegowda.github.io") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/summary/nmt/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/intro.html"><img src="/summary/nmt/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">Research Summaries</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/intro.html"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/summary/nmt/search.html">Search</a></li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://github.com/thammegowda/summary"
        >
          
          GitHub repository
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__divider"></li>
        
      
      
        <li><h2 class="c-sidebar__title">Neural Machine Translation</li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/00-background/01-background.html"
        >
          
            1.
          
          Background
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/summary/nmt/00-background/02-variational-auto-encoder.html"
                >
                  
                    1.1
                  
                  Variational Auto Encoder
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/summary/nmt/00-background/03-gans.html"
                >
                  
                    1.2
                  
                  GANs
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/01-sup/01-beginning.html"
        >
          
            2.
          
          Supervised MT
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/02-semisup/01-semisup.html"
        >
          
            3.
          
          Semi Supervised MT
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/summary/nmt/03-unsup/01-unsupervised-nmt.html"
        >
          
            4.
          
          Un-Supervised MT
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            

            <div class="c-textbook__content">
              <div class="search-content__inner-wrap">
    <input type="text" id="lunr_search" class="search-input" tabindex="-1" placeholder="'Enter your search term...''" />
    <div id="results" class="results"></div>
</div>

<script>
    // Add the lunr store since we will now search it
    var store = [{
        "title": "Background",
        
        "excerpt":
            "Background   Auto Encoder learns representations by the technique of compression (aka dim reduction) folled by decompression. The compact representation is often called “code”. For an high demnsiomal data $X$, we define encoder as a parameterized function $Enc_\\phi : X \\rightarrow F$  Where $F$ is a compact representation in lower dimensions. Simultaneously, we pair this compressing function with a decompressing function to reconstruct the original data.  $Dec_\\psi : F \\rightarrow X$   Usually, these compressor and decompressors are lossy. Ideally we want the reconstruction to as close as possible to input (thats the learning objective of learning the parameters $\\phi$ and $\\psi$). The objective here is to minimize the reconstruction error, which can be expressed as $\\Vert X − Dec_\\psi(Enc_\\phi(X))\\Vert^2$.   However, this is still very abstract - we dont know how input $X$ is, how many parameters in $\\phi$ and $\\psi$, and how to set them. Usually, they are all depends on the input $X$.   We want to approximate these parameterized functions (i.e. approximate parameters  $\\phi$ and $\\psi$). We define an optimizer that can directly adjust these parameters by utilizing a set of samples (training data). The chosen dataset, and the chosen optimization algorithm can have a noticable effect (or bias) in the estimation of parameters, which inturn affects the end objective. Another implicit objective is to be generic enough, i.e. if given another set of samples (test set), the parameters should still behave as desired. From the theoretical point of view, this is maximum likelihood estimation (MLE) over a set of samples. In simple words, its the task of finding maximum-likelihood (i.e. most likely) values of $\\phi$ and $\\psi$ on a give set of samples (training set) while also keeping the estimation generic enough to be used on other unseen samples (test set). In summary, we have two objectives:      Objective 1: Better reconstruction i.e. $Dec_\\psi(Enc_\\phi(X))$ should be very similar to input $X$   Objective 2: Better generalization: This method should also work on unseen samples.   Deep learning / Neural networks are one way of accomplishing these defined objectives. So $Enc_\\phi$ and $Dec_\\psi$ are two neural networks. The nice thing about networks is that we can connect two or more of them to form a bigger network. So Autoencoder in our context is a big network which has two sub networks - encoder and decoder.   Lets take images as our input, since they are simpler and visually inspectable. They can be resized to have fixed number of dimensions without losing much of information (after all, we are in the context of lossy compressions and approximations of those lossy compressions). The reason for chosing images over linguistic structures (scuh as sentences) is that - unlike images sentences cannot be easily resized to have fixed dimensions, and they have temporal property - so more complicated!   In the computer vision we use a kind of networks called convolusion neural networks (with some more fancier additions) to approximate $Enc_\\phi$ and $Dec_\\psi$. Experimental evidance has shown that this kind of Encoding and Decoding easily satisfies Reconstruction objectives, however fails to satisfy generalization objective.   Denoising Auto Encoder In this setup, the input $X$ is intentionally corrupted. The task of encoder is to reconstruct the clean input from noisy input. This task is some what challenging than the vanilla task, so forces the models to learn better representations.  ",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/00-background/01-background.html",
        "teaser":null},{
        "title": "Variational Auto Encoder",
        
        "excerpt":
            "Variational Auto Encoders   (Vanilla) Autoencoders have the intermediate latent space which interface Encoder and Decoder. This latent space is a $\\mathbb{R}$eal valued space. The Encoder can map input $X$ to any arbitrary real valued $F$ from which the decoder can reconstruct original input.   Next, we look inside this latent space $F$ and make some assumptions(for fun, better understanding, or just simpler interfacing of encoder and decoder). Maybe the latent space need not to be any arbitrary real valued space. Maybe its a random variable with its distributional parameters that can be either guessed, or learned. What we have now with this assumption is Variational AutoEncoder(VAE). VAEs have a strong assumption on the latent space.   Lets start off with a guess: mabe the latent space representation is a multivariate Gaussian(Mean, Variance), normalized to Zero mean and Unit variance i.e. $N(0, I)$. There has been some work to relax this constraint by learning these parameters of latent space distribution as well. (NOTE: Not detouring to those line of work here.)   Since we made a strong assumption on the latent space, we need to enforce it on encoders and decoders. Here is an idea: We just need to enforce the encoder to obey this assumption, and there is no change needed to the decoder since it will work with the constrained encoder just like before. How to do this? We modify the loss function slightly to enforce the assumption.   Here is a walk through:      Assumption: There is a distribution $z$ from which all samples are generated, i.e $p(z)$ are its parameters. It could generate all samples in the universe, and those could be infinite! We have a small subset of those samples at hand $x \\in X$ which we call our training set.   However, we dont know the parameters of full $p(z)$, so we are trying to estimate it.            $p(z)$ which can generate almost anything in the universe is hard to estimate.       Let’s take a constrained version of this: we have a finite set of samples $x \\in X$ (our dataset), so the joint $P(z, x)$ sounds easier to approximate and validate. $p(z)$ can be seen as marginalization of $p(z, x)$ over an infinite set of samples $x$ in the universe (that sounds hard).           We know $p(z, x) = p(x) \\times p(z \\vert x) = p(z) \\times p(x \\vert z)$.            Interesting Observation: suppose we have a way to force $p(z \\vert x) \\approx p(z)$ (we do, as seen later), then $p(x \\vert z) \\approx p(x)$           Encoder $Enc_\\phi : x \\rightarrow z$ is same as $Enc_\\phi(z \\vert x)$ an approximation to $p(z \\vert x)$, where $p(z \\vert x)$ itself an approximation to $p(z)$   Decoder $Dec_\\psi : z \\rightarrow x$ is same as $Dec_\\psi(x \\vert z)$ the reconstruction from latent space. this is an approximation to $p(x \\vert z)$ which itself an approximation to $p(x)$. this approximation holds iff $Enc_\\phi(z \\vert x) \\approx p(x \\vert z) \\approx p(z)$   $ReconstructionLoss$ is same as vainilla autoencoder, and it depends on the task at hand:            squared error: $\\Vert X − Dec_\\psi(Enc_\\phi(X))\\Vert^2$       cross entropy (in some variation): $-log Dec_\\psi(Enc_\\phi(X))$ (for time series classification like text sequence)           We want the $Enc_\\phi(z \\vert x)$ to be close to $p(z)$ (or $p(z \\vert x)$). We dont have $p(z)$  or $p(z \\vert x)$. Maybe we can learn an approximation to it: $p(z) \\approx p_\\theta(z)$. We can learn $p_\\theta(z)$ later, for now (the simple model) take a safe guess $p_\\theta(z) = N(0, I)$   $TotalLoss = D_{KL} ( Enc_\\phi(z \\vert x) \\Vert p_\\theta(z)) + ReconstructionLoss$   $D_{KL}$ is the KL divergence between what encoder produces $Enc_\\phi(z \\vert x)$ and  the assumed+learned latent space $p_\\theta(z)$. Diveregence measure enfoces the distributions to look similar, i.e. $Enc_\\phi(z \\vert x) \\approx p_\\theta(z)$. We can replace $D_{KL}$ with something else as long as it forces the distributions to look same. It can be any other divergence measure if that helps in the robust convergence of loss during training.     For example, $Wasserstein Distance-1$ can be used if constraining the parameters in $Enc_\\phi$ to a certain range. Recently $Wasserstein Distance-1$ with some constraints on $Enc_\\phi$ has become popular.  TODO: read more Wasserstein AutoEncoder.   ",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/00-background/02-variational-auto-encoder.html",
        "teaser":null},{
        "title": "GANs",
        
        "excerpt":
            "Generative Adversarial Networks(GANs)   As we seen in the previous section, the Variation Auto Encoder made a strong assumption of latent space:     Latent space can be modelled as a random distribution   That random distribution is a multivariate gaussian with zero mean and unit variance.   The role of an encoder in the variational autoencoder is to convert any input $x$ into some point in the space of $\\mathcal{N}(0, I)$. In other words, if we have a sample set of examples (images, text), each of them can be deterministically mapped (using an encoder) to a point in the space $\\mathcal{N}(0, I)$. We have one unique point for each example in our finite dataset. However, there are inifinite number of points in $\\mathcal{N}(0, I)$, there fore we could in theory generate infinite number of examples using the $Dec\\psi(z)$ where $z$ is a sample from $\\mathcal{N}(0, I)$.   Since the desired output space of the encoder is known/assumed to be $\\mathcal{N}(0, I)$, and we do know how to generate a random point from that space, then why do we need an encoder? Especially if we are just interested in generation from this known latent space, we can take out the encoder, since all we need is a decoder to convert noise to an example. In this model-without-Encoder, let us call the Decoder as Generator since it is more meaningful name (and thats what the authors called it).   To begin with, we obtain some random samples from $\\mathcal{N}(0,I)$ (call it $noise$) and pass it to the Decoder aka Generator. The generator would always generate something irrespective of what we provide as input. But the real challenge here is making the generator generate only useful outputs. Since there is no encoder in this model, we dont know which specific example was mapped to the noise, so we are unable to measure reconstruction loss. Most of the time it is not even reconstruction, it is a novel/never-before-seen construction. This model needs a clever way to force the generator produce only useful outputs.   (Goodfellow et al., 2014) had a clever and interesting idea which is now commonly referred as adversarial training.      I see an interesting connection to algorithms complexity theory: Suppose if we have a blackbox to efficiently decide a given solution is correct or wrong (i.e. NP aka Non-deterministic Polynomial), searching a correct solution of a NP (hard) problem can be done efficiently using this deciding blackbox. Here, if we can have a blackbox to decide the generated output as Good or Bad, we can use it to search the good parameters of Generator.    The module that can signal whether a generated output is good or bad is called Descriminator. Since there are no readily available descriminator, it needs to be learned too. The Descriminator can also be treated as a parameterized function, and learn its parameters. Since we are implementing parameterized functions as neural networks, we can connect Decoder aka Generator to Descriminator to form a bigger network and optimize both of them as one end objective. There is more detail involved to the optimization technique, it is more interesting than it looks at surface.   Walk through for how the generator-descriminator network can be adversarially optimized     the authors called generator as $g_\\theta$, to be consistent with previous notes on VAEs, $g_\\theta = Gen_\\psi = Dec_\\psi$.       The assumed latent space $\\mathcal{N}(0, I)$   Sample a radom point from the latent space, say $s$. There is an example  corresponding to this $s$. The generator can generate it.            the sample space $\\mathcal{N}(0, I)$ has infinite samples. Even if we obtain a very large finite set of known examples the probability of $s$ corresponding to a known example in the set is negligible. That is okay, since we are not after exact reconstruction loss           Generator: $\\tilde{z} = Gen_\\psi(s)$ Here $\\tilde{z}$ is a generated example   Descriminator:  $r=Desc_w(\\tilde{z})$ . The range of $r=[0, 1]$ Where 0 mean bad/useless/fake and 1 means good/useful/real.   Simultaneously train the $Gen_\\psi$ and $Desc_w$. To simultaneously optimize all the parameters, we will need a set of realistic examples.   Notation: Let $z$ be the real example read from training set (whose space is $P_*$), and $\\tilde{z}= Gen_\\psi(s)$  with space $P_{\\tilde{z}}$   A well trained Descriminator should have $Desc_w(z)$ to be close to 1 (since they are real), and $Desc_w(\\tilde{z})$ should be close to $0$ since are generated (or fake)   To get to the well trained Descriminator, maximimize $E_{z \\sim P_*} \\space [ Desc_w(z) ] - E_{\\tilde{z} \\sim P_z} \\space [ Desc_w(\\tilde{z}) ]$   Meanwhile the Generator’s task is to get better at making the descriminator’s task harder. Therefore, the objective for Generator is to do exactly opposite of Descriminator (a true adversary), which is to minimize $E_{z \\sim P_*} \\space [ Desc_w(z) ] - E_{\\tilde{z} \\sim P_z} \\space [ Desc_w(\\tilde{z}) ]$   Min-max optimization.      TODO: this is incomplete. Read Min-max optimization   ",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/00-background/03-gans.html",
        "teaser":null},{
        "title": "Supervised MT",
        
        "excerpt":
            "Beginning: Things that lead to NMT  ====================   NMT Models     LSTM seq2seq   LSTM  seq2seq with attention   More attention types   GRUs   CNNs   Transformer  ",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/01-sup/01-beginning.html",
        "teaser":null},{
        "title": "Semi Supervised MT",
        
        "excerpt":
            "Semi Supervised NMT   TODO     Back Translation   External Language Models  ",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/02-semisup/01-semisup.html",
        "teaser":null},{
        "title": "Un-Supervised MT",
        
        "excerpt":
            "Un Supervised NMT Training NMT models without using a single parallel data sounds like a daunting challenge at first - how is it even possible? There has been surprising amount of progress made on this challenge recently (2017-2019). There were efforts to train Statistical MT systems without using bitext, notably the paper titled Deciphering foreign language (Ravi &amp; Knight, 2011). When it comes to NMT, we see a natural progression of the task: first, learn word translations without any data, followed by a way to take advanatge of them for to sentence translations. Unsupervised Word alignments, done in embedding space, is generally a two step process: Learn monolingual embeddings seperately Learn the transformation function mapping one space to another. There are plenty of ways to learn word embeddings: Word2vec, Fasttext, Glove, etc Each have their own advantages (and some drawbacks). The task is straight forward application of distributional hypothesis on monolingual data. The second step, learning transformation function for mapping one to another is some what interesting for translation community. Progression of learning word embedding alignments: Using a dictionary of word translations to learn the transformation matrix. Exploiting Similarities among Languages for Machine Translation (Mikolov, Le, &amp; Sutskever, 2013) Much smaller dictionaries: as small as 25 pairs. In many cases those pairs can be automatically obtained, eg: numbers, names etc. See Learning bilingual word embeddings with (almost) no bilingual data (Artetxe, Labaka, &amp; Agirre, 2017) More fancier techniques for learning word alignments: Use adversarial training. See Word translation without parallel data (Conneau, Lample, Ranzato, Denoyer, &amp; Jégou, 2017) . The implementation of (Conneau, Lample, Ranzato, Denoyer, &amp; Jégou, 2017) ‘s approch is made available on github as facebookresearch/MUSE and it is more popular. These unsupervised approaches can sometimes perform better than supervised approach. Here is a visual interepretation of embedding alignment (taken from their github repo): In summary, unsupervised word alignments exploited these two phenomenons: Words having similar meaning appear in similar context across languages There is a linear mapping from one embedding vector space to another which can be easily learned Once we had these automatically aligned word embeddings of good enough quality, the next problem to tackle was: “how to do sentence translation without parallel data?” 1. Unsupervised neural machine translation (Artetxe, Labaka, Agirre, &amp; Cho, 2017) Crosslingual embeddings: Skipgram word2vec, 10 neg sample, 10 ctx window, 300 dims. Then aligned using (Artetxe, Labaka, &amp; Agirre, 2017) NMT Architecture: 2 layer Bi-GRU encoder, 2 layers GRU dec, 600 hid dim, 300 dim embs, general attention Both directions: Source -&gt; Target and Target -&gt; Source Shared encoder for both source and target, decoders are seperate. Encoder embeddings are fixed. Decoders are let to evolve. Vocabularies are separate for both languages. So embedding matrices are seperate (but aligned) Denoising: for a seq of $N$ toks, $N/2$ swaps are performed as nimitation of oise. Without noise, copy task is too trivial, AEs doesnt learn any useful representation. On-the-fly back translation: they use the model to generate translation (in the inference mode with greedy dec) then reconstruct from the translation. Backtranslation is much noisier form than random word swaps. Switch the training steps: between Denoise L1 to L1; Denoise L2 to L2; Cycle via BackTranslation: L1 -&gt; L2 -&gt; L1; Cycle via BackTranslation: L2 -&gt; L1 -&gt; L2. Trained on short seqs: 50 or fewer toks (after BPE) Crosslingual embeddings: Took 4-5 days to train. Results on WMT: Fr-en 15.6; De-en 10.2 2. Unsupervised machine translation using monolingual corpora only (Lample, Conneau, Denoyer, &amp; Ranzato, 2017) Cross lingual embeddings from (Conneau, Lample, Ranzato, Denoyer, &amp; Jégou, 2017) i.e. MUSE. Learn to reconstruct in both langs from the shared latent space Denoising auto encoder, reconstruct from a noisy input (This paper has good citations to relevant work; semi supervised, autoencoder etc) LSTM with 300 dims, 3 layers. Two models: Src-to-tgt and tgt-to-src models: all encoder layers are shared, all decoder layers are shared. So only generator matrix is different for both the languages. They actually have two models, LSTM layers are shared. Denoising auto encoders: drop words, shuffle the order of tokens (upper bound k on how many timesteps max a token can move). They found 10% word drop and k=3 be good parameters Cross-domain loss (aka cycle loss) x -&gt; C(x) -&gt; y -&gt; C(y) -&gt; x’. C is corruption operator. Similarly, y -&gt; C(y) -&gt; x -&gt; C(x) -&gt; y’. Adversarial training: Encoding of x and y sentences should be indistinguishable Final objective is: is linear combination of following AE src-&gt;src AE tgt-&gt;tgt CycleLoss src-&gt;tgt-&gt;src CycleLoss tgt-&gt;src-&gt;src Adversarial Loss Results On WMT fr-en 14.3 while supervised is 26.1; de-en is 13.3 while supervvised is 25.6 3. Unsupervised neural machine translation with weight sharing (Yang, Chen, Wang, &amp; Xu, 2018) Artetxe et al 2017 use a shared encoder; but seperate decoders. Lample et al 2017 share a single encoder and single decoder (fully shared). Conjecture: Fully shared is not good, since languages have different syntax; completely different is also not good, since forcing common latent space is hard. We want to something in the middle: partial sharing. Encoder: share the higher layers; allow the lower layers (close to embeddings) to be different Decoder share the lower layers; allow the higher layers (close to generator module) to be different Question: Decoder too has input embedding too just like encoder, but they are okay shared? Maybe: since word embeddings are in common latent space Two kinds of adversarial discriminators are built in: Prior work (Yang et all 2017, same first author): “Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets” Loss is modified to include regular loss + GAN reward Two Discriminators: Local (encoded repr of source &amp; target are indistinguishable), Global (model generated sentences are indistinguishable from human generated sentences) Slight improvement in BLEU (+1.0) compared to Lample et al 2017 4. Phrase-Based &amp; Neural Unsupervised Machine Translation(Lample, Ott, Conneau, Denoyer, &amp; Ranzato, 2018) Simplification from lample et al 2017 and Artetxe et al 2017. In essence these three work for unsupervised...",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/03-unsup/01-unsupervised-nmt.html",
        "teaser":null},{
        "title": "Home",
        
        "excerpt":
            "Neural Machine Translation (NMT)   Research summaries on Neural Machine Translation.   The summary is divided into three sub-sections:     Background   Supervised (WIP)   Semi-supervised (WIP)   Unsupervised   Authors (Primary contributors)     Thamme Gowda (tg@isi.edu)   (You are welcome here!)   Secondary contributors     (You are welcome here!)  ",
        "categories": [],
        "tags": [],
        "url": "https://thammegowda.github.io/summary/nmt/intro.html",
        "teaser":null},]
</script>
              <nav class="c-page__nav">
  

  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
